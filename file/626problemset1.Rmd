---
title: "626 Midterm 1"
author: "Wenjing Li"
date: "2023-04-15"
output: pdf_document
df_print: paged
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(flextable)
library(ggplot2)
library(readxl)
```

# Problem 1-3
For problem 1-3 see <https://github.com/Lynn9358/BIOSTAT626-Midterm1>

# Problem 4 Baseline algorithm 
## Task 1 
For the first task, logistic regression model is used as baseline algorithm with all the variable concluded. Since the result of prediction is the expection instead of binary value, a threshold as 0 was set to transfer the expection to binary value. The result shows high accuracy, however, it is not time effective.
The model used: $$logit(status1) = \beta^T X, where\quad X = 1,x_1,x_2,...,x_{561}$$
Here is an example of prediction using baseline logistic regression model, 3-fold cv result are presented in terms of confusion matrix and accuracy:
```{r , echo=FALSE}
b1a = matrix(c(1369,1,0,1150), nrow = 2)
b1b = matrix(c(1593,2,4,1044), nrow = 2)
b1c = matrix(c(1515,1,14,1074), nrow = 2)
colnames(b1a) = c(0,1)
rownames(b1a) =c(0,1)
colnames(b1b) = c(0,1)
rownames(b1b) =c(0,1)
colnames(b1c) = c(0,1)
rownames(b1c) =c(0,1)
as.table(b1a)
as.table(b1b)
as.table(b1c)
acc_b1 = c(0.9996032, 0.9977299, 0.9942396, 0.9971909)
acc_b1 = as.table(acc_b1)
names(acc_b1) = c(1,2,3,"Mean")
acc_b1
```



## Task 2
For task2, I used svm  with linear kernel as baseline algorithm, where the tolerance is defalted value as = 0.01, and family is multinomial. Compare other algorithms. It show higher accuracy, but it still need to be improved.
Here is an example of prediction using baseline svm model, 3-fold cv result are presented in terms of confusion matrix and accuracy:
```{r , include=FALSE}
f2a = as.matrix(read_excel("~/Downloads/f2a.xlsx", col_names = FALSE))

f2b = as.matrix(read.table("~/Downloads/f2b.txt", quote="\""))
f2c = as.matrix(read.table("~/Downloads/f2c.txt", quote="\""))
colnames(f2a) = c(1:7)
rownames(f2a) =c(1:7)
colnames(f2b) = c(1:7)
rownames(f2b) =c(1:7)
colnames(f2c) = c(1:7)
rownames(f2c) =c(1:7)
acc_f2 = c(0.9170635, 0.9197881, 0.9304916, 0.9224477)
acc_f2 = as.table(acc_f2)
names(acc_f2) = c(1,2,3,"Mean")

```

```{r , echo=FALSE}

f2a
f2b
f2c
acc_f2
```

# Problem 5 final algorithm
## Task 1 
For task 1, logistic regression model shows high accuracy, however it has some problem with  static cases, some of the static cases are misclassified as dynamic cases, therefore I changed the threshold from 0 to -5 , and the accuracy improved.
Here is an example of prediction using final logistic regression model, 3-fold cv result are presented in terms of confusion matrix and accuracy:
```{r , echo=FALSE}
f1a = matrix(c(1369,1,0,1150), nrow = 2)
f1b = matrix(c(1593,2,4,1044), nrow = 2)
f1c = matrix(c(1515,1,10,1078), nrow = 2)
colnames(f1a) = c(0,1)
rownames(f1a) =c(0,1)
colnames(f1b) = c(0,1)
rownames(f1b) =c(0,1)
colnames(f1c) = c(0,1)
rownames(f1c) =c(0,1)
as.table(f1b)
as.table(f1b)
as.table(f1c)
acc_f1 = c(0.9996032, 0.9977299, 0.9957757, 0.9977029)
names(acc_b1) = c(1,2,3,"Mean")
acc_b1
```

## Task 2 
For task2, I used Lagrange multiplier SVM, and add the classification result from task 1 as a new variable, which improves the accuracy.
Here is an example of prediction using final Lagrange multiplier SVM model, 3-fold cv result are presented in terms of confusion matrix and accuracy:

```{r , include=FALSE}
b2a = as.matrix(read_excel("~/Downloads/b2a.xlsx", col_names = FALSE))
b2b = as.matrix(read_excel("~/Downloads/b2b.xlsx", col_names = FALSE))
b2c = as.matrix(read_excel("~/Downloads/b2c.xlsx", col_names = FALSE))
colnames(b2a) = c(1:7)
rownames(b2a) =c(1:7)
colnames(b2b) = c(1:7)
rownames(b2b) =c(1:7)
colnames(b2c) = c(1:7)
rownames(b2c) =c(1:7)
acc_b2 = c(0.9277778, 0.9250851, 0.9385561, 0.930473)
acc_b2 = as.table(acc_b2)
names(acc_b2) = c(1,2,3,"Mean")

```

```{r , echo=FALSE}

b2a
b2b
b2c
acc_b2
```


This is the change in accuracy for task 1 and task 2, the accuracy for task 1 did not change much because it is high at baseline, and the improvement for task 2 is greater.
```{r , echo=FALSE, fig.width=10, fig.align = 'center'}
result = matrix( c(0.99719, 0.99770, 0.9224477, 0.930473), ncol = 2)
colnames(result) = c("task1", "task2")
rownames(result) = c("baseline", "final")
barplot(result, main = "Change in accuracy", beside=TRUE, legend=TRUE,ylim = c(0.9,1))


```



# Problem 6 Leaderboard performance
```{r , echo=FALSE}
acc = matrix(c(0,"1.000","1.000","1.000","1.000",0.937,0.926,0.958,0.965,0.965),ncol = 2)
acc = as.data.frame(acc)
colnames(acc) <- c("Task1","Task2")
rownames(acc) <- c("1st trial","2ed trial","3rd trial","4th trial","5th trial")
acc
```
For the first submission on task 1, dynamic cases and static cases get opposite value, which resulted in 0 accuracy, in the next submission, changed the value of it.

In the first and second submission in task2, I used random forest algorithm and knn algorithm, and I split the training data randomly into training set and test set without considering the subject index, therefore, it has high accuracy in the training data, and it does not truly reflect the model performance. In the next submissions, I test my model based on sets that split based on subject index, which gives a more accurate reflect of model performance. I tried LDA and svm with radical,linear and polynomial kernel. Among which,svm with linear kernel does best in the dataset, which means the data can be nearly perfect split by dividing different planes. Therefore I worked on improve the baseline svm model performance by using a svm with Lagrangian multiplier and add the first task result as a new variable.

# Problem 7 Comment & Further improvement
Comment: The accuracy of binary classification result is 1.000, however, both of the model are greatly time-consuming. The accuracy of binary classification result is 0.965, however, it can be improved by further adjusting the parameters and by ensemble learning.

Further improvement: For task 1, the logistic regression model can be greatly improved by stepwise, which will eliminate the parameters in the model and reduce the time and cache it used. For task 2, the alpha in Lagrangian multiplier can be optimized to get better performs, also I notice that svm does better in static cases while some other model, such as knn does better in dynamic cases, there are also some models performs best in specific cases. To improve the performance of the final model, we can apply ensemble learning, such as boosting and Bayes optimal classifier.


